# --- SageMaker setup ---
# Import libraries Sagemaker (Container)

import boto3 # AWS SDK for Python. Used for direct S3 actions, clients, etc. (use it to upload CSVs) 
import os # filesystem / env var utilities.
import sagemaker # high-level SageMaker Python SDK. Provides Session, Estimator, helper classes and deployment tools
from sagemaker import get_execution_role # Convenience that returns the IAM role ARN when you run inside a SageMaker managed environment (Studio, notebook instance)
from sagemaker.serializers import CSVSerializer # Used by predictors to serialize inputs when calling a deployed endpoint (text/csv)
from sagemaker.inputs import TrainingInput # Helper that wraps an S3 URI and metadata (content type, input mode) for channels passed to .fit()
from sagemaker.sklearn import SKLearn # Higher-level shortcut to the Scikit-Learn estimator in the SDK

# Get the SageMaker session and the execution role from the SageMaker domain
sess = sagemaker.Session() # Creates an object that holds config (default S3 bucket, region) and helpers for uploading, describing jobs, etc.
role = get_execution_role() # An IAM role ARN that SageMaker uses to access S3, CloudWatch, ECR, etc. The role must have the right permissions (S3 write/read, SageMaker actions)

bucket = 'script-mode-xgb-demo' # Update with the name of a bucket that is already created in S3
prefix = 'demo-xgb-hyperopt' # The name of the folder that will be created in the S3 bucket

print(f"Session bucket: {bucket}")
print(f"Prefix: {prefix}")
print("âœ… SageMaker session ready")


import pandas as pd
from sklearn.model_selection import train_test_split

# load dataset
df = pd.read_csv('data/training_data.csv')
df_copy = df.copy()
df_copy.head()


# --- Feature Engineering ---
df_copy['feature_2_9_13'] = df_copy['feature_2'] * df_copy['feature_9'] * df_copy['feature_13']
df_copy['feature_9_x_13'] = df_copy['feature_9'] * df_copy['feature_13']

selected_features = [
    'feature_2_9_13',
    'feature_9_x_13',
    'feature_11',
    'feature_18',
    'feature_2',
    'target'
]
df_xgb = df_copy[selected_features]
df_xgb.head()


# --- Defining features and target ---
X_full = df_xgb.drop(columns='target')
y_full = df_xgb['target']

# Test Split
# 80/20
X_temp, X_test, y_temp, y_test = train_test_split(X_full, y_full, test_size=0.2, random_state=12345)

# Validation Set (0.25 of 0.8 = 0.2)
X_train, X_valid, y_train, y_valid = train_test_split(X_temp, y_temp, test_size=0.25, random_state=12345)
# The test size is 25% of the training data (80%), which is 20% of the full dataset
# 60 training /20 validation / 20 test 

# --- Export to CSV files ---
train_df = pd.concat([X_train, y_train], axis=1)
valid_df = pd.concat([X_valid, y_valid], axis=1)
test_df  = pd.concat([X_test, y_test], axis=1)

train_df.to_csv("train.csv", index=False)
valid_df.to_csv("validation.csv", index=False)
test_df.to_csv("test.csv", index=False)

print("âœ… Local CSVs created successfully:")
print(f" - train.csv ({train_df.shape})")
print(f" - validation.csv ({valid_df.shape})")
print(f" - test.csv ({test_df.shape})")


# --- Upload training and validation data to the S3 bucket ---
# Path used: s3://<bucket>/<prefix>/train/train.csv, etc.
boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')
boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')
boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'test/test.csv')).upload_file('test.csv')

# Objects with the location of the training, testing and validation data in the S3 provided 
# content_type is the input parsing: how the training container reads your input files once it downloads them from S3
s3_input_train = TrainingInput(
    s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv'
)
s3_input_validation = TrainingInput(
    s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv'
)
s3_input_test = TrainingInput(
    s3_data='s3://{}/{}/test/'.format(bucket, prefix), content_type='csv'
)


# --- Create Estimator ---
from sagemaker.sklearn.estimator import SKLearn

sklearn_estimator = SKLearn(
    entry_point='train.py', # File that SageMaker will run inside the training container (Script Mode)
    instance_type='ml.m5.xlarge', # Compute instance type used by the training job. This determines cost and CPU/GPU availability
    framework_version='1.2-1', # Specify the SKLearn container version. The container is prebuilt; it will run train.py inside it
    role=role, # IAM role SageMaker will assume to read S3 input, write S3 output, push logs, etc
    base_job_name='xgb-hyperopt-demo', # Prefix used when SageMaker creates the unique training job name
    py_version='py3',
    dependencies=['requirements.txt']  # Instructs the training container to pip-install packages from requirements.txt into the container before running train.py

# --- Launch training job ---
print("ðŸš€ Launching training job...")
sklearn_estimator.fit({ # .fit starts all the SageMaker service
    'train': s3_input_train,
    'validation': s3_input_validation,
    'test': s3_input_test
})
print("âœ… Training job launched successfully!")


# --- Where the model is (S3 SageMaker bucket) ---

import sagemaker

sess = sagemaker.Session()
job_name = "xgb-hyperopt-demo-2025-10-16-01-27-09-622"  # last job name
desc = sess.describe_training_job(job_name)
print("âœ… Training job status:", desc["TrainingJobStatus"])
print("ðŸ“¦ Model S3 Path:", desc["ModelArtifacts"]["S3ModelArtifacts"])



